{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COND_HMF_CHECKPOINTS/TEST_FINALS_SIMPLE_BATCHED_model_save_nsim1_cond_sim_quijote_nsd128_nc4_nsh128_mass_rockstar_200c_stype_cic_nsimperbatch32_nbatches16\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# import pickle as pk\n",
    "import numpy as np\n",
    "import torch\n",
    "dev = torch.device(\"cuda\")\n",
    "import torch.optim as optim\n",
    "# from torch.distributions import MultivariateNormal\n",
    "# from torch.distributions import Normal\n",
    "root_dir = '/mnt/home/spandey/ceph/AR_NPE/'\n",
    "os.chdir(root_dir)\n",
    "# import colossus\n",
    "import sys, os\n",
    "# append the root_dir to the path\n",
    "sys.path.append(root_dir)\n",
    "from nf.combined_models import COMBINED_Model\n",
    "from nf.all_models import *\n",
    "from nf.utils_data_prep import *\n",
    "# from tqdm import tqdm\n",
    "# import pyyaml\n",
    "from colossus.cosmology import cosmology\n",
    "params = {'flat': True, 'H0': 67.11, 'Om0': 0.3175, 'Ob0': 0.049, 'sigma8': 0.834, 'ns': 0.9624}\n",
    "cosmo = cosmology.setCosmology('myCosmo', **params)\n",
    "# get halo mass function:\n",
    "from colossus.lss import mass_function\n",
    "from tqdm import tqdm\n",
    "    \n",
    "import yaml\n",
    "\n",
    "run_config_name = sys.argv[1]\n",
    "# run_config_name = 'run_test_128_condQ_pylianscic_200c_lgMmin5e13.yaml'\n",
    "\n",
    "with open(\"/mnt/home/spandey/ceph/AR_NPE/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "    config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "config_sims = config['sim_settings']\n",
    "ji_array = np.arange(int(config_sims['nsims']))\n",
    "ns_d = config_sims['ns_d']\n",
    "nb = config_sims['nb']\n",
    "nax_d =  ns_d // nb\n",
    "nf = config_sims['nf']\n",
    "layers_types = config_sims['layers_types']\n",
    "nc = 0\n",
    "for jl in range(len(layers_types)):\n",
    "    if layers_types[jl] == 'cnn':\n",
    "        nc += 1\n",
    "    elif layers_types[jl] == 'res':\n",
    "        nc += 2\n",
    "    else:\n",
    "        raise ValueError(\"layer type not supported\")\n",
    "\n",
    "z_all = config_sims['z_all']\n",
    "z_all_FP = config_sims['z_all_FP']\n",
    "ns_h = config_sims['ns_h']\n",
    "nax_h = ns_h // nb\n",
    "cond_sim = config_sims['cond_sim']\n",
    "\n",
    "nsims_per_batch = config_sims['nsims_per_batch']\n",
    "nbatches_train = config_sims['nbatches_train']\n",
    "\n",
    "mass_type = config_sims['mass_type']\n",
    "lgMmin = config_sims['lgMmin']\n",
    "lgMmax = config_sims['lgMmax']\n",
    "stype = config_sims['stype']\n",
    "rescale_sub = config_sims['rescale_sub']\n",
    "lgMmincutstr = config_sims['lgMmincutstr']\n",
    "\n",
    "config_net = config['network_settings']\n",
    "hidden_dim_MAF = config_net['hidden_dim_MAF']\n",
    "learning_rate = config_net['learning_rate']\n",
    "K_M1 = config_net['K_M1']\n",
    "B_M1 = config_net['B_M1']\n",
    "nflows_M1_NSF = config_net['nflows_M1_NSF']\n",
    "\n",
    "K_Mdiff = config_net['K_Mdiff']\n",
    "B_Mdiff = config_net['B_Mdiff']\n",
    "nflows_Mdiff_NSF = config_net['nflows_Mdiff_NSF']\n",
    "\n",
    "base_dist_M1 = config_net['base_dist_M1']\n",
    "base_dist_Mdiff = config_net['base_dist_Mdiff']\n",
    "ngauss_M1 = config_net['ngauss_M1']\n",
    "\n",
    "changelr = config_net['changelr']\n",
    "ksize = nf\n",
    "nfeature_cnn = config_net['nfeature_cnn']\n",
    "nout_cnn = 4 * nfeature_cnn\n",
    "if cond_sim == 'fastpm':\n",
    "    ninp = len(z_all_FP)\n",
    "elif cond_sim == 'quijote':\n",
    "    ninp = len(z_all)\n",
    "else:\n",
    "    raise ValueError(\"cond_sim not supported\")\n",
    "\n",
    "num_cond = nout_cnn + ninp\n",
    "\n",
    "df_d_all_train, df_d_all_nsh_train, df_Mh_all_train, df_Nh_train = load_density_halo_data_NGP(\n",
    "    ji_array, ns_d, nb, nf, nc, z_all, ns_h,sdir='/mnt/home/spandey/ceph/Quijote/data_NGP_self',\n",
    "    stype=stype, mass_type=mass_type, lgMmincutstr = lgMmincutstr\n",
    "    )\n",
    "\n",
    "# # Prepare the density and halo data\n",
    "return_dict_train = prep_density_halo_cats_batched(\n",
    "    df_d_all_train, df_d_all_nsh_train, df_Mh_all_train, df_Nh_train, nsims=nsims_per_batch,\n",
    "    nbatches = nbatches_train, Mmin=lgMmin, Mmax=lgMmax, rescaleM_sub=rescale_sub\n",
    "    )\n",
    "\n",
    "if cond_sim == 'fastpm':\n",
    "    df_d_all_train_FP, df_d_all_nsh_train_FP, df_Mh_all_train_FP, df_Nh_train_FP = load_density_halo_data_NGP(\n",
    "        ji_array, ns_d, nb, nf, nc, z_all_FP, ns_h, stype=stype,sdir='/mnt/home/spandey/ceph/Quijote/data_NGP_self/fastpm'\n",
    "        )\n",
    "\n",
    "    # # Prepare the density and halo data\n",
    "    return_dict_train_FP = prep_density_halo_cats_batched(\n",
    "        df_d_all_train_FP, df_d_all_nsh_train_FP, df_Mh_all_train_FP, df_Nh_train_FP, nsims=nsims_per_batch,\n",
    "        nbatches = nbatches_train, Mmin=lgMmin, Mmax=lgMmax, rescaleM_sub=rescale_sub\n",
    "        )\n",
    "else:\n",
    "    return_dict_train_FP = None\n",
    "\n",
    "\n",
    "\n",
    "lgM_array = np.linspace(lgMmin, lgMmax, 1000)\n",
    "M_array = 10**lgM_array\n",
    "if '200c' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, 0, mdef = '200c', model = 'tinker08', q_out = 'dndlnM')\n",
    "if 'vir' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, 0, mdef = 'vir', model = 'tinker08', q_out = 'dndlnM')    \n",
    "if 'fof' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, 0, mdef = 'fof', model = 'bhattacharya11', q_out = 'dndlnM')\n",
    "lgM_rescaled = rescale_sub + (lgM_array - lgMmin)/(lgMmax-lgMmin)\n",
    "\n",
    "int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
    "hmf_pdf = hmf/int_val\n",
    "# define the cdf of the halo mass function\n",
    "hmf_cdf = np.zeros_like(hmf_pdf)\n",
    "for i in range(len(hmf_cdf)):\n",
    "    hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n",
    "\n",
    "\n",
    "num_cond_Ntot = num_cond\n",
    "model_Ntot = SumGaussModel(\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_Ntot,\n",
    "    ngauss=return_dict_train['ngauss_Nhalo'],\n",
    "    mu_all=return_dict_train['mu_all'],\n",
    "    sig_all=return_dict_train['sig_all'],\n",
    "    base_dist='pl_exp'    \n",
    "    )\n",
    "\n",
    "num_cond_M1 = num_cond + 1\n",
    "# if conditioned on fastpm we will also give the fastpm fof M1 halos and its mask as conditional\n",
    "if cond_sim == 'fastpm':\n",
    "    num_cond_M1 += 2\n",
    "\n",
    "model_M1 = NSF_M1_CNNcond(\n",
    "    K=K_M1,\n",
    "    B=B_M1,\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_M1,\n",
    "    nflows=nflows_M1_NSF,\n",
    "    base_dist=base_dist_M1,\n",
    "    ngauss=ngauss_M1,\n",
    "    lgM_rs_tointerp=lgM_rescaled,\n",
    "    hmf_pdf_tointerp=hmf_pdf,\n",
    "    hmf_cdf_tointerp=hmf_cdf    \n",
    "    )\n",
    "\n",
    "ndim_diff = return_dict_train['M_diff_halos_all_norm_masked'][0].shape[2]\n",
    "num_cond_Mdiff = num_cond + 2\n",
    "model_Mdiff = NSF_Mdiff_CNNcond(\n",
    "    dim=ndim_diff,\n",
    "    K=K_Mdiff,\n",
    "    B=B_Mdiff,\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_Mdiff,\n",
    "    nflows=nflows_Mdiff_NSF,\n",
    "    base_dist=base_dist_Mdiff,\n",
    "    mu_pos=True\n",
    "    )\n",
    "\n",
    "ndim = ndim_diff + 1\n",
    "model = COMBINED_Model(\n",
    "    None,\n",
    "    model_Mdiff,\n",
    "    # None,\n",
    "    model_M1,\n",
    "    model_Ntot,\n",
    "    ndim,\n",
    "    ksize,\n",
    "    ns_d,\n",
    "    ns_h,\n",
    "    nb,\n",
    "    ninp,\n",
    "    nfeature_cnn,\n",
    "    nout_cnn,\n",
    "    layers_types=layers_types,\n",
    "    act='tanh',\n",
    "    padding='valid',\n",
    "    sep_Ntot_cond=True,\n",
    "    sep_M1_cond=True,\n",
    "    sep_Mdiff_cond=True,\n",
    "    num_cond_Ntot = num_cond_Ntot,\n",
    "    num_cond_M1 = num_cond_M1,\n",
    "    num_cond_Mdiff = num_cond_Mdiff\n",
    "    )\n",
    "\n",
    "model.to(dev)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_all_it = []\n",
    "loss_min = 1e20\n",
    "epoch_tot_counter = 0\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=200, verbose=True, cooldown=200, min_lr=1e-8)\n",
    "\n",
    "save_bestfit_model_name = 'COND_HMF_CHECKPOINTS/TEST_FINALS_SIMPLE_BATCHED_model_save_nsim' + str(len(ji_array)) + \\\n",
    "                            '_cond_sim_' + cond_sim + \\\n",
    "                            '_nsd' + str(ns_d) + '_nc' + str(nc) + '_nsh' + str(ns_h) + '_mass_' + mass_type + \\\n",
    "                            '_stype_' + stype + '_nsimperbatch' + str(nsims_per_batch) + '_nbatches' + str(nbatches_train)\n",
    "print(save_bestfit_model_name)\n",
    "\n",
    "try:\n",
    "    print('loading bestfit model')\n",
    "    bestfit_model = (torch.load(save_bestfit_model_name))\n",
    "    model.load_state_dict(bestfit_model['state_dict'])\n",
    "    optimizer.load_state_dict(bestfit_model['optimizer'])\n",
    "    scheduler.load_state_dict(bestfit_model['scheduler'])\n",
    "    loss_min = bestfit_model['loss_min']\n",
    "    loss = bestfit_model['loss']\n",
    "    lr = bestfit_model['lr']\n",
    "    epoch_tot_counter = bestfit_model['epoch_tot_counter']\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "except:\n",
    "    print('no bestfit model found, running new')\n",
    "    pass\n",
    "    epoch_tot_counter = 0\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class LoadCustomData(Dataset):\n",
    "    def __init__(self, return_dict, return_dict_FP, all_gpu=False):\n",
    "        if return_dict_FP is not None:\n",
    "            self.cond_tensor = torch.Tensor(np.array(return_dict_FP['df_d_all']))\n",
    "            cond_nsh = np.moveaxis(np.array(return_dict_FP['df_d_all_nsh']), 2, 5)\n",
    "            self.cond_tensor_nsh = torch.Tensor((cond_nsh)).reshape(-1, nsims_per_batch * (nax_h ** 3), ninp)\n",
    "        else:\n",
    "            self.cond_tensor = torch.Tensor(np.array(return_dict['df_d_all']))\n",
    "            cond_nsh = np.moveaxis(np.array(return_dict['df_d_all_nsh']), 2, 5)\n",
    "            self.cond_tensor_nsh = torch.Tensor((cond_nsh)).reshape(-1, nsims_per_batch * (nax_h ** 3), ninp)\n",
    "\n",
    "        self.mask_tensor_M1_train = torch.Tensor(np.array(return_dict['mask_M1'])).reshape(-1, nsims_per_batch * (nax_h**3))\n",
    "        self.mask_tensor_Mdiff_train = torch.Tensor((np.array(return_dict['mask_M_diff']))).reshape(-1, nsims_per_batch * (nax_h**3), ndim_diff)\n",
    "\n",
    "        self.X_M1 = torch.Tensor((np.array(return_dict['M1_halos_all_norm']))).reshape(-1, nsims_per_batch * (nax_h**3), 1)\n",
    "        self.X_Nhalo = torch.Tensor(np.array(return_dict['Nhalo_train_mg_arr'])).reshape(-1, nsims_per_batch * (nax_h**3), 1)\n",
    "        self.X_Mdiff = torch.Tensor(np.array(return_dict['M_diff_halos_all_norm_masked'])).reshape(-1, nsims_per_batch * (nax_h**3),ndim_diff)\n",
    "        self.Nhalos_truth_tensor = torch.Tensor(((np.array(return_dict['N_halos_all'])))).reshape(-1, nsims_per_batch * (nax_h**3), 1)\n",
    "\n",
    "        if return_dict_FP is not None:\n",
    "            self.mask_tensor_M1_train_FP = torch.Tensor(np.array(return_dict_FP['mask_M1'])).reshape(-1, nsims_per_batch * (nax_h**3))\n",
    "            self.X_M1_FP = torch.Tensor((np.array(return_dict_FP['M1_halos_all_norm']))).reshape(-1, nsims_per_batch * (nax_h**3), 1)\n",
    "        else:\n",
    "            self.mask_tensor_M1_train_FP = None\n",
    "            self.X_M1_FP = None\n",
    "        if all_gpu:\n",
    "            self.cond_tensor = self.cond_tensor.cuda(dev)\n",
    "            self.cond_tensor_nsh = self.cond_tensor_nsh.cuda(dev)\n",
    "            self.mask_tensor_M1_train = self.mask_tensor_M1_train.cuda(dev)\n",
    "            self.mask_tensor_Mdiff_train = self.mask_tensor_Mdiff_train.cuda(dev)\n",
    "            self.X_M1 = self.X_M1.cuda(dev)\n",
    "            self.X_Nhalo = self.X_Nhalo.cuda(dev)\n",
    "            self.X_Mdiff = self.X_Mdiff.cuda(dev)\n",
    "            self.Nhalos_truth_tensor = self.Nhalos_truth_tensor.cuda(dev)\n",
    "            if return_dict_FP is not None:\n",
    "                self.mask_tensor_M1_train_FP = self.mask_tensor_M1_train_FP.cuda(dev)\n",
    "                self.X_M1_FP = self.X_M1_FP.cuda(dev)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cond_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond_tensor = (self.cond_tensor[idx,...])\n",
    "        cond_tensor_nsh = self.cond_tensor_nsh[idx,...]\n",
    "        mask_tensor_M1_train = self.mask_tensor_M1_train[idx,...]\n",
    "        mask_tensor_Mdiff_train =self.mask_tensor_Mdiff_train[idx,...]\n",
    "        X_M1 = self.X_M1[idx,...]\n",
    "        X_Nhalo = self.X_Nhalo[idx,...]\n",
    "        X_Mdiff = self.X_Mdiff[idx,...]\n",
    "        Nhalos_truth_tensor = self.Nhalos_truth_tensor[idx,...]\n",
    "        if self.mask_tensor_M1_train_FP is not None:\n",
    "            mask_tensor_M1_train_FP = self.mask_tensor_M1_train_FP[idx,...]\n",
    "            X_M1_FP = self.X_M1_FP[idx,...]\n",
    "        else:\n",
    "            mask_tensor_M1_train_FP = torch.Tensor([-9999])\n",
    "            X_M1_FP = torch.Tensor([-9999])\n",
    "\n",
    "        return (cond_tensor, cond_tensor_nsh, mask_tensor_M1_train, mask_tensor_Mdiff_train, X_M1, X_Nhalo,\n",
    "                 X_Mdiff, Nhalos_truth_tensor, mask_tensor_M1_train_FP, X_M1_FP)\n",
    "\n",
    "\n",
    "config_train = config['train_settings']\n",
    "batch_size = config_train['batch_size_DL']\n",
    "all_gpu = config_train['all_gpu']\n",
    "dataset = LoadCustomData(return_dict_train, return_dict_train_FP, all_gpu=all_gpu)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=1 - all_gpu, num_workers=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:00<1:28:55,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -23.71881 , at epoch: 0 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [00:49<1:19:16,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -23.86937 , at epoch: 100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [01:39<1:19:56,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -23.94418 , at epoch: 200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [02:27<1:15:44,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -23.98763 , at epoch: 300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [03:14<1:13:30,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.0148 , at epoch: 400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [04:03<1:14:17,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.03323 , at epoch: 500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 601/10000 [04:50<1:13:18,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.04642 , at epoch: 600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 701/10000 [05:38<1:11:45,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.05637 , at epoch: 700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 801/10000 [06:27<1:12:43,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.06435 , at epoch: 800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 901/10000 [07:14<1:14:05,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.071 , at epoch: 900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1001/10000 [08:02<1:10:09,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.077 , at epoch: 1000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1101/10000 [08:49<1:09:41,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.08255 , at epoch: 1100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1201/10000 [09:37<1:08:45,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.08827 , at epoch: 1200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1301/10000 [10:26<1:10:55,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.09491 , at epoch: 1300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1401/10000 [11:13<1:07:24,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.10376 , at epoch: 1400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 1936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1501/10000 [12:01<1:06:29,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.11763 , at epoch: 1500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1601/10000 [12:48<1:05:59,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.14394 , at epoch: 1600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1701/10000 [13:36<1:05:30,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.20206 , at epoch: 1700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1801/10000 [14:24<1:10:35,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.30831 , at epoch: 1800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1901/10000 [15:12<1:08:22,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.4221 , at epoch: 1900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2001/10000 [16:00<1:04:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.45257 , at epoch: 2000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2101/10000 [16:47<1:01:35,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.45993 , at epoch: 2100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2201/10000 [17:35<1:00:43,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.46333 , at epoch: 2200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2301/10000 [18:24<1:07:47,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.46568 , at epoch: 2300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2401/10000 [19:11<1:00:44,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.46749 , at epoch: 2400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2501/10000 [19:59<57:39,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.46906 , at epoch: 2500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2601/10000 [20:47<57:10,  2.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47038 , at epoch: 2600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2701/10000 [21:35<56:33,  2.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.4718 , at epoch: 2700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2801/10000 [22:23<1:07:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47304 , at epoch: 2800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2901/10000 [23:10<55:56,  2.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47424 , at epoch: 2900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3001/10000 [23:59<54:20,  2.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47545 , at epoch: 3000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3101/10000 [24:46<54:24,  2.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47658 , at epoch: 3100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [25:35<53:32,  2.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.4778 , at epoch: 3200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3301/10000 [26:24<59:43,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.47895 , at epoch: 3300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3401/10000 [27:11<52:49,  2.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48005 , at epoch: 3400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 3936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3501/10000 [27:59<52:25,  2.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48109 , at epoch: 3500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3601/10000 [28:47<49:40,  2.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48214 , at epoch: 3600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3701/10000 [29:35<49:34,  2.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48309 , at epoch: 3700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3801/10000 [30:24<58:03,  1.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48408 , at epoch: 3800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3901/10000 [31:11<48:06,  2.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48485 , at epoch: 3900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4001/10000 [32:00<46:39,  2.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48561 , at epoch: 4000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4101/10000 [32:47<47:35,  2.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48655 , at epoch: 4100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4201/10000 [33:36<44:33,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48737 , at epoch: 4200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4301/10000 [34:25<47:40,  1.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48816 , at epoch: 4300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4401/10000 [35:12<44:18,  2.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48863 , at epoch: 4400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 4936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [36:01<42:56,  2.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.48961 , at epoch: 4500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4601/10000 [36:49<44:59,  2.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.49029 , at epoch: 4600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4701/10000 [37:37<41:50,  2.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.49099 , at epoch: 4700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/10000 [38:26<41:27,  2.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.49173 , at epoch: 4800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4901/10000 [39:14<39:51,  2.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.49251 , at epoch: 4900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [40:02<38:20,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.493 , at epoch: 5000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5101/10000 [40:52<57:07,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.4947 , at epoch: 5100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5201/10000 [41:39<37:25,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.49971 , at epoch: 5200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5301/10000 [42:28<36:40,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.50862 , at epoch: 5300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5401/10000 [43:15<36:19,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.50986 , at epoch: 5400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5501/10000 [44:03<34:42,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51084 , at epoch: 5500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5601/10000 [44:51<34:41,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51197 , at epoch: 5600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5701/10000 [45:39<34:44,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51283 , at epoch: 5700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5801/10000 [46:28<33:07,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51344 , at epoch: 5800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5901/10000 [47:15<32:25,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51361 , at epoch: 5900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6001/10000 [48:04<31:39,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51536 , at epoch: 6000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6101/10000 [48:52<46:46,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51632 , at epoch: 6100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6201/10000 [49:40<29:33,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51737 , at epoch: 6200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6301/10000 [50:29<29:27,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.51867 , at epoch: 6300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6401/10000 [51:16<28:08,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52002 , at epoch: 6400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6501/10000 [52:05<27:35,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52093 , at epoch: 6500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6601/10000 [52:53<32:51,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52248 , at epoch: 6600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6701/10000 [53:40<25:49,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52322 , at epoch: 6700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6801/10000 [54:29<25:28,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52391 , at epoch: 6800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6901/10000 [55:17<24:07,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52437 , at epoch: 6900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7001/10000 [56:06<23:33,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52486 , at epoch: 7000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7101/10000 [56:55<24:48,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52533 , at epoch: 7100 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7201/10000 [57:43<22:05,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52576 , at epoch: 7200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7301/10000 [58:32<21:18,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52606 , at epoch: 7300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7401/10000 [59:20<20:27,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52655 , at epoch: 7400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7501/10000 [1:00:09<19:38,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52693 , at epoch: 7500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7701/10000 [1:01:47<18:19,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52718 , at epoch: 7700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7801/10000 [1:02:36<17:42,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52843 , at epoch: 7800 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8001/10000 [1:04:13<15:40,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52927 , at epoch: 8000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8201/10000 [1:05:51<18:55,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52955 , at epoch: 8200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8301/10000 [1:06:39<13:42,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.52961 , at epoch: 8300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8401/10000 [1:07:28<12:40,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53013 , at epoch: 8400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8501/10000 [1:08:15<11:50,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53081 , at epoch: 8500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8701/10000 [1:09:50<10:10,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53199 , at epoch: 8700 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8901/10000 [1:11:27<08:42,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53241 , at epoch: 8900 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9001/10000 [1:12:15<08:01,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53321 , at epoch: 9000 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9201/10000 [1:13:52<09:34,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.5339 , at epoch: 9200 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9301/10000 [1:14:40<05:36,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53445 , at epoch: 9300 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9401/10000 [1:15:29<04:43,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.535 , at epoch: 9400 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 9936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9501/10000 [1:16:16<03:58,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53576 , at epoch: 9500 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 10036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9601/10000 [1:17:05<03:08,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53614 , at epoch: 9600 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 10136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:20:18<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -24.53709 , at epoch: 9999 learning rate: 0.001 , train_Ntot: 1 train_M1: 0 , train_Mdiff: 0 , epoch_tot_counter: 10535\n",
      "loading bestfit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:00<1:42:54,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.37224 , at epoch: 0 learning rate: 0.001 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 10536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [01:06<1:59:11,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.46094 , at epoch: 100 learning rate: 0.001 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 10636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [02:09<1:44:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10736: reducing learning rate of group 0 to 5.0000e-04.\n",
      "saving bf at: , with loss: -0.48302 , at epoch: 200 learning rate: 0.0005 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 10736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [03:13<1:40:20,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.49158 , at epoch: 300 learning rate: 0.0005 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 10836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [04:17<1:40:42,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.4983 , at epoch: 400 learning rate: 0.0005 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 10936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [05:21<1:39:31,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.50307 , at epoch: 500 learning rate: 0.0005 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 601/10000 [06:25<1:38:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.50666 , at epoch: 600 learning rate: 0.0005 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 602/10000 [06:26<1:38:12,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11137: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 701/10000 [07:29<1:38:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5082 , at epoch: 700 learning rate: 0.00025 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 801/10000 [08:32<1:35:56,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.50959 , at epoch: 800 learning rate: 0.00025 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 901/10000 [09:37<1:41:19,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51089 , at epoch: 900 learning rate: 0.00025 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1001/10000 [10:42<1:35:39,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51211 , at epoch: 1000 learning rate: 0.00025 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1003/10000 [10:43<1:34:16,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11538: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1101/10000 [11:46<1:32:33,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51274 , at epoch: 1100 learning rate: 0.000125 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1201/10000 [12:51<1:31:40,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51335 , at epoch: 1200 learning rate: 0.000125 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1301/10000 [13:54<1:31:40,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51398 , at epoch: 1300 learning rate: 0.000125 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1401/10000 [14:58<1:29:43,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5146 , at epoch: 1400 learning rate: 0.000125 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 11936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1404/10000 [15:00<1:28:53,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11939: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1501/10000 [16:02<1:28:04,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51492 , at epoch: 1500 learning rate: 6.25e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1601/10000 [17:06<1:35:55,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51524 , at epoch: 1600 learning rate: 6.25e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1701/10000 [18:10<1:26:53,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51556 , at epoch: 1700 learning rate: 6.25e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1801/10000 [19:14<1:26:53,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51589 , at epoch: 1800 learning rate: 6.25e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1805/10000 [19:16<1:24:45,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12340: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1901/10000 [20:18<1:24:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51606 , at epoch: 1900 learning rate: 3.125e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2001/10000 [21:22<1:23:11,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51623 , at epoch: 2000 learning rate: 3.125e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2101/10000 [22:26<1:22:43,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51641 , at epoch: 2100 learning rate: 3.125e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2201/10000 [23:31<1:22:07,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5166 , at epoch: 2200 learning rate: 3.125e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2206/10000 [23:34<1:43:18,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12741: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2301/10000 [24:35<1:37:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5167 , at epoch: 2300 learning rate: 1.5625e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2401/10000 [25:39<1:20:25,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51679 , at epoch: 2400 learning rate: 1.5625e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 12936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2501/10000 [26:44<1:19:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51689 , at epoch: 2500 learning rate: 1.5625e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2601/10000 [27:48<1:17:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.517 , at epoch: 2600 learning rate: 1.5625e-05 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2607/10000 [27:52<1:16:33,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13142: reducing learning rate of group 0 to 7.8125e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2701/10000 [28:52<1:16:16,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51705 , at epoch: 2700 learning rate: 7.8125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2801/10000 [29:56<1:16:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51711 , at epoch: 2800 learning rate: 7.8125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2901/10000 [31:00<1:15:21,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51717 , at epoch: 2900 learning rate: 7.8125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3001/10000 [32:05<1:35:19,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51722 , at epoch: 3000 learning rate: 7.8125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3008/10000 [32:09<1:14:59,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13543: reducing learning rate of group 0 to 3.9063e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3101/10000 [33:09<1:13:57,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51726 , at epoch: 3100 learning rate: 3.90625e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [34:13<1:11:21,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51729 , at epoch: 3200 learning rate: 3.90625e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3301/10000 [35:17<1:10:21,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51732 , at epoch: 3300 learning rate: 3.90625e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3401/10000 [36:21<1:09:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51735 , at epoch: 3400 learning rate: 3.90625e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 13936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3409/10000 [36:26<1:07:52,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13944: reducing learning rate of group 0 to 1.9531e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3501/10000 [37:25<1:08:17,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51737 , at epoch: 3500 learning rate: 1.953125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3601/10000 [38:29<1:07:11,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51739 , at epoch: 3600 learning rate: 1.953125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3701/10000 [39:34<1:37:38,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51741 , at epoch: 3700 learning rate: 1.953125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3801/10000 [40:38<1:06:48,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51743 , at epoch: 3800 learning rate: 1.953125e-06 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3810/10000 [40:44<1:04:20,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14345: reducing learning rate of group 0 to 9.7656e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3901/10000 [41:42<1:04:10,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51744 , at epoch: 3900 learning rate: 9.765625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4001/10000 [42:46<1:02:13,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51745 , at epoch: 4000 learning rate: 9.765625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4101/10000 [43:50<1:01:22,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51746 , at epoch: 4100 learning rate: 9.765625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4201/10000 [44:54<1:01:48,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51747 , at epoch: 4200 learning rate: 9.765625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4211/10000 [45:00<1:00:09,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14746: reducing learning rate of group 0 to 4.8828e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4301/10000 [45:58<59:24,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51748 , at epoch: 4300 learning rate: 4.8828125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4401/10000 [47:02<58:57,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51748 , at epoch: 4400 learning rate: 4.8828125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 14936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [48:08<1:01:31,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51749 , at epoch: 4500 learning rate: 4.8828125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4601/10000 [49:12<57:01,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51749 , at epoch: 4600 learning rate: 4.8828125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4612/10000 [49:19<55:29,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15147: reducing learning rate of group 0 to 2.4414e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4701/10000 [50:17<55:27,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5175 , at epoch: 4700 learning rate: 2.44140625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/10000 [51:21<53:51,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5175 , at epoch: 4800 learning rate: 2.44140625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4901/10000 [52:25<53:47,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.5175 , at epoch: 4900 learning rate: 2.44140625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [53:29<52:12,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51751 , at epoch: 5000 learning rate: 2.44140625e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5013/10000 [53:37<55:42,  1.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15548: reducing learning rate of group 0 to 1.2207e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5101/10000 [54:34<1:15:43,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51751 , at epoch: 5100 learning rate: 1.220703125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5201/10000 [55:39<51:55,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51751 , at epoch: 5200 learning rate: 1.220703125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5301/10000 [56:43<49:21,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51751 , at epoch: 5300 learning rate: 1.220703125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5401/10000 [57:47<48:32,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51751 , at epoch: 5400 learning rate: 1.220703125e-07 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 15936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5414/10000 [57:55<47:51,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15949: reducing learning rate of group 0 to 6.1035e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5501/10000 [58:51<48:17,  1.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 5500 learning rate: 6.103515625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5601/10000 [59:56<46:02,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 5600 learning rate: 6.103515625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5701/10000 [1:01:00<45:17,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 5700 learning rate: 6.103515625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5801/10000 [1:02:05<56:39,  1.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 5800 learning rate: 6.103515625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5815/10000 [1:02:14<43:24,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16350: reducing learning rate of group 0 to 3.0518e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5901/10000 [1:03:09<43:57,  1.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 5900 learning rate: 3.0517578125e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6101/10000 [1:05:17<40:59,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6100 learning rate: 3.0517578125e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6201/10000 [1:06:21<39:24,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6200 learning rate: 3.0517578125e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6216/10000 [1:06:30<39:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16751: reducing learning rate of group 0 to 1.5259e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6401/10000 [1:08:28<37:26,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6400 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 16936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6501/10000 [1:09:32<36:17,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6500 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6601/10000 [1:10:37<38:48,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6600 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6801/10000 [1:12:45<33:15,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6800 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6901/10000 [1:13:49<32:41,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 6900 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7001/10000 [1:14:52<31:31,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 7000 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7101/10000 [1:15:56<30:13,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 7100 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7301/10000 [1:18:05<36:29,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51752 , at epoch: 7300 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 17836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7501/10000 [1:20:12<27:26,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 7500 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7601/10000 [1:21:16<25:38,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 7600 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7701/10000 [1:22:21<24:09,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 7700 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7801/10000 [1:23:25<23:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 7800 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7901/10000 [1:24:29<21:51,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 7900 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8001/10000 [1:25:34<31:47,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8000 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8101/10000 [1:26:38<21:05,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8100 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8201/10000 [1:27:42<18:59,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8200 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8301/10000 [1:28:46<18:04,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8300 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8401/10000 [1:29:50<17:05,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8400 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 18936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8501/10000 [1:30:54<15:47,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8500 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8601/10000 [1:31:58<14:36,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8600 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8701/10000 [1:33:02<13:33,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8700 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8801/10000 [1:34:07<13:19,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8800 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8901/10000 [1:35:11<11:47,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 8900 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9001/10000 [1:36:16<10:25,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9000 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9101/10000 [1:37:20<09:29,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9100 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9201/10000 [1:38:24<08:19,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9200 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9301/10000 [1:39:28<07:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9300 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 19836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9501/10000 [1:41:37<05:35,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9500 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9601/10000 [1:42:42<04:14,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9600 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9701/10000 [1:43:46<03:06,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51753 , at epoch: 9700 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9801/10000 [1:44:50<02:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51754 , at epoch: 9800 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9901/10000 [1:45:54<01:02,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51754 , at epoch: 9900 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:46:58<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.51754 , at epoch: 9999 learning rate: 1.52587890625e-08 , train_Ntot: 0 train_M1: 1 , train_Mdiff: 0 , epoch_tot_counter: 20535\n",
      "loading bestfit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:02<7:19:26,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: 1.89605 , at epoch: 0 learning rate: 0.001 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 20536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [05:02<8:10:16,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: 0.02107 , at epoch: 100 learning rate: 0.001 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 20636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [10:08<9:14:16,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.03365 , at epoch: 200 learning rate: 0.001 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 20736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 226/10000 [11:24<8:13:51,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20761: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [15:15<8:21:05,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.03764 , at epoch: 300 learning rate: 0.0005 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 20836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [20:21<8:13:36,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.03881 , at epoch: 400 learning rate: 0.0005 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 20936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [25:28<8:03:16,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.03973 , at epoch: 500 learning rate: 0.0005 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 21036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 601/10000 [30:32<7:51:21,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.04027 , at epoch: 600 learning rate: 0.0005 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 21136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 627/10000 [31:51<7:54:24,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21162: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 701/10000 [35:56<9:47:40,  3.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.04058 , at epoch: 700 learning rate: 0.00025 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 21236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 801/10000 [42:17<9:43:46,  3.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -0.04085 , at epoch: 800 learning rate: 0.00025 , train_Ntot: 0 train_M1: 0 , train_Mdiff: 1 , epoch_tot_counter: 21336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 837/10000 [44:19<8:05:18,  3.18s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         X_M1_FP_jd \u001b[39m=\u001b[39m X_M1_FP_jd\u001b[39m.\u001b[39mcuda(dev)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     X_Mdiff_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     X_M1_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     X_Nhalo_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     cond_x\u001b[39m=\u001b[39;49mcond_tensor_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     cond_x_nsh\u001b[39m=\u001b[39;49mcond_tensor_nsh_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     mask_Mdiff_truth_all\u001b[39m=\u001b[39;49mmask_tensor_Mdiff_train_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     mask_M1_truth_all\u001b[39m=\u001b[39;49mmask_tensor_M1_train_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     Nhalos_truth_all\u001b[39m=\u001b[39;49mNhalos_truth_tensor_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     use_Ntot_samples\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     use_M1_samples\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     train_Ntot\u001b[39m=\u001b[39;49mtrain_Ntot,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     train_M1\u001b[39m=\u001b[39;49mtrain_M1,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     train_Mdiff\u001b[39m=\u001b[39;49mtrain_Mdiff,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m# x_Mdiff_FP=X_Mdiff_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     x_M1_FP\u001b[39m=\u001b[39;49mX_M1_FP_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39m# x_Ntot_FP=X_Nhalo_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# Nhalos_truth_all_FP=Nhalos_truth_tensor_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# mask_Mdiff_truth_all_FP=mask_tensor_Mdiff_train_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     mask_M1_truth_all_FP\u001b[39m=\u001b[39;49mmask_tensor_M1_train_FP_jd             \n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ceph/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ceph/users/spandey/AR_NPE/nf/combined_models.py:215\u001b[0m, in \u001b[0;36mCOMBINED_Model.forward\u001b[0;34m(self, x_Mdiff, x_M1, x_Ntot, cond_x, cond_x_nsh, mask_Mdiff_truth_all, mask_M1_truth_all, Nhalos_truth_all, use_Ntot_samples, use_M1_samples, reg_M1, train_Ntot, train_M1, train_Mdiff, x_Mdiff_FP, x_M1_FP, x_Ntot_FP, Nhalos_truth_all_FP, mask_Mdiff_truth_all_FP, mask_M1_truth_all_FP)\u001b[0m\n\u001b[1;32m    213\u001b[0m             loss_Mdiff \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMdiff_model\u001b[39m.\u001b[39mforward(x_Mdiff[jb], cond_inp_Mdiff, mask_Mdiff_truth)\n\u001b[1;32m    214\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m             loss_Mdiff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMdiff_model\u001b[39m.\u001b[39;49mforward(x_Mdiff[jb], cond_inp_Mdiff, mask_Mdiff_truth)\n\u001b[1;32m    216\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(loss_Ntot \u001b[39m+\u001b[39m loss_M1 \u001b[39m+\u001b[39m loss_M1reg \u001b[39m+\u001b[39m loss_Mdiff)\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/ceph/users/spandey/AR_NPE/nf/all_models.py:642\u001b[0m, in \u001b[0;36mNSF_Mdiff_CNNcond.forward\u001b[0;34m(self, x_inp, cond_inp, mask)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39m# if len(x.shape) > 1:\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m#     x = x[:, 0]\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m log_det_all_jd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(x_inp\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    643\u001b[0m log_det_all_jd \u001b[39m=\u001b[39m log_det_all_jd\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    644\u001b[0m \u001b[39mfor\u001b[39;00m jf \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnflows):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepochs_Ntot_only = config_train['nepochs_Ntot_only']\n",
    "nepochs_Ntot_M1_only = config_train['nepochs_Ntot_M1_only']\n",
    "nepochs_all = config_train['nepochs_all']\n",
    "\n",
    "\n",
    "nepochs_array = [nepochs_Ntot_only, nepochs_Ntot_M1_only, nepochs_all]\n",
    "train_Ntot_array = [1, 1, 1]\n",
    "train_M1_array = [0, 1, 1 ]\n",
    "train_Mdiff_array = [0, 0, 1]\n",
    "\n",
    "\n",
    "for jn in (range(len(nepochs_array))):\n",
    "    loss_min = 1e20\n",
    "    torch.cuda.empty_cache()\n",
    "    ninit = 0\n",
    "    nepochs = nepochs_array[jn]\n",
    "    if nepochs > 0:\n",
    "        train_Ntot = train_Ntot_array[jn]\n",
    "        train_M1 = train_M1_array[jn]\n",
    "        train_Mdiff = train_Mdiff_array[jn]\n",
    "\n",
    "        if jn > 0:\n",
    "            print('loading bestfit model')\n",
    "            bestfit_model = (torch.load(save_bestfit_model_name))\n",
    "            model.load_state_dict(bestfit_model['state_dict'])\n",
    "            optimizer.load_state_dict(bestfit_model['optimizer'])\n",
    "            scheduler.load_state_dict(bestfit_model['scheduler'])\n",
    "            # loss_min = bestfit_model['loss_min']\n",
    "            loss = bestfit_model['loss']\n",
    "            lr = bestfit_model['lr']\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = learning_rate\n",
    "\n",
    "        for jt in tqdm(range(nepochs)):\n",
    "            for jd in range(len(dataloader)):\n",
    "                torch.cuda.empty_cache()\n",
    "                optimizer.zero_grad()\n",
    "                cond_tensor_jd, cond_tensor_nsh_jd, mask_tensor_M1_train_jd, mask_tensor_Mdiff_train_jd, X_M1_jd, \\\n",
    "                    X_Nhalo_jd, X_Mdiff_jd, Nhalos_truth_tensor_jd, mask_tensor_M1_train_FP_jd, X_M1_FP_jd = next(iter(dataloader))\n",
    "\n",
    "                if cond_sim == 'quijote':               \n",
    "                    mask_tensor_M1_train_FP_jd = None\n",
    "                    X_M1_FP_jd = None\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                if 1-all_gpu:\n",
    "                    cond_tensor_jd = cond_tensor_jd.cuda(dev)\n",
    "                    cond_tensor_nsh_jd = cond_tensor_nsh_jd.cuda(dev)\n",
    "                    mask_tensor_M1_train_jd = mask_tensor_M1_train_jd.cuda(dev)\n",
    "                    mask_tensor_Mdiff_train_jd = mask_tensor_Mdiff_train_jd.cuda(dev)\n",
    "                    X_M1_jd = X_M1_jd.cuda(dev)\n",
    "                    X_Nhalo_jd = X_Nhalo_jd.cuda(dev)\n",
    "                    X_Mdiff_jd = X_Mdiff_jd.cuda(dev)\n",
    "                    Nhalos_truth_tensor_jd = Nhalos_truth_tensor_jd.cuda(dev)\n",
    "                    if mask_tensor_M1_train_FP_jd is not None:\n",
    "                        mask_tensor_M1_train_FP_jd = mask_tensor_M1_train_FP_jd.cuda(dev)\n",
    "                        X_M1_FP_jd = X_M1_FP_jd.cuda(dev)\n",
    "                    torch.cuda.empty_cache()\n",
    "                                \n",
    "                loss = model(\n",
    "                    X_Mdiff_jd,\n",
    "                    X_M1_jd,\n",
    "                    X_Nhalo_jd,\n",
    "                    cond_x=cond_tensor_jd,\n",
    "                    cond_x_nsh=cond_tensor_nsh_jd,\n",
    "                    mask_Mdiff_truth_all=mask_tensor_Mdiff_train_jd,\n",
    "                    mask_M1_truth_all=mask_tensor_M1_train_jd,\n",
    "                    Nhalos_truth_all=Nhalos_truth_tensor_jd,\n",
    "                    use_Ntot_samples=False,\n",
    "                    use_M1_samples=False,\n",
    "                    train_Ntot=train_Ntot,\n",
    "                    train_M1=train_M1,\n",
    "                    train_Mdiff=train_Mdiff,\n",
    "                    # x_Mdiff_FP=X_Mdiff_FP,\n",
    "                    x_M1_FP=X_M1_FP_jd,\n",
    "                    # x_Ntot_FP=X_Nhalo_FP,\n",
    "                    # Nhalos_truth_all_FP=Nhalos_truth_tensor_FP,\n",
    "                    # mask_Mdiff_truth_all_FP=mask_tensor_Mdiff_train_FP,\n",
    "                    mask_M1_truth_all_FP=mask_tensor_M1_train_FP_jd             \n",
    "                    )\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            epoch_tot_counter += 1\n",
    "            if (np.mod(jt, int(nepochs / 100)) == 0) or (jt == nepochs - 1):\n",
    "                if float(loss.cpu().detach().numpy()) < loss_min:\n",
    "                    loss_min = float(loss.cpu().detach().numpy())\n",
    "                    print('saving bf at:', ', with loss:', np.round(loss_min, 5), ', at epoch:', jt, \n",
    "                          'learning rate:', optimizer.param_groups[0]['lr'], ', train_Ntot:', train_Ntot, \n",
    "                          'train_M1:', train_M1, ', train_Mdiff:', train_Mdiff, ', epoch_tot_counter:', epoch_tot_counter)\n",
    "                    lr=optimizer.param_groups[0]['lr']\n",
    "                    # print(loss_min, lr)\n",
    "                    state = {'loss_min': loss_min, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                              'scheduler': scheduler.state_dict(), 'loss':loss, 'lr':lr, 'epoch_tot_counter':epoch_tot_counter}\n",
    "\n",
    "                    torch.save(\n",
    "                        state, save_bestfit_model_name\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bestfit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:04<12:51:04,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving bf at: , with loss: -20.10082 , at epoch: 0 learning rate: 0.001 , train_Ntot: 1 train_M1: 1 , train_Mdiff: 1 , epoch_tot_counter: 21373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/10000 [00:23<10:41:23,  3.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         X_M1_FP_jd \u001b[39m=\u001b[39m X_M1_FP_jd\u001b[39m.\u001b[39mcuda(dev)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     X_Mdiff_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     X_M1_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     X_Nhalo_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     cond_x\u001b[39m=\u001b[39;49mcond_tensor_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     cond_x_nsh\u001b[39m=\u001b[39;49mcond_tensor_nsh_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     mask_Mdiff_truth_all\u001b[39m=\u001b[39;49mmask_tensor_Mdiff_train_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     mask_M1_truth_all\u001b[39m=\u001b[39;49mmask_tensor_M1_train_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     Nhalos_truth_all\u001b[39m=\u001b[39;49mNhalos_truth_tensor_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     use_Ntot_samples\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     use_M1_samples\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     train_Ntot\u001b[39m=\u001b[39;49mtrain_Ntot,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     train_M1\u001b[39m=\u001b[39;49mtrain_M1,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     train_Mdiff\u001b[39m=\u001b[39;49mtrain_Mdiff,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m# x_Mdiff_FP=X_Mdiff_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     x_M1_FP\u001b[39m=\u001b[39;49mX_M1_FP_jd,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39m# x_Ntot_FP=X_Nhalo_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# Nhalos_truth_all_FP=Nhalos_truth_tensor_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# mask_Mdiff_truth_all_FP=mask_tensor_Mdiff_train_FP,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     mask_M1_truth_all_FP\u001b[39m=\u001b[39;49mmask_tensor_M1_train_FP_jd             \n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Brusty/mnt/home/spandey/ceph/AR_NPE/notebooks/TEST_ROCKSTAR_RUNS/RUN_ROCKSTAR_200c_pylianscic.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ceph/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ceph/users/spandey/AR_NPE/nf/combined_models.py:215\u001b[0m, in \u001b[0;36mCOMBINED_Model.forward\u001b[0;34m(self, x_Mdiff, x_M1, x_Ntot, cond_x, cond_x_nsh, mask_Mdiff_truth_all, mask_M1_truth_all, Nhalos_truth_all, use_Ntot_samples, use_M1_samples, reg_M1, train_Ntot, train_M1, train_Mdiff, x_Mdiff_FP, x_M1_FP, x_Ntot_FP, Nhalos_truth_all_FP, mask_Mdiff_truth_all_FP, mask_M1_truth_all_FP)\u001b[0m\n\u001b[1;32m    213\u001b[0m             loss_Mdiff \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMdiff_model\u001b[39m.\u001b[39mforward(x_Mdiff[jb], cond_inp_Mdiff, mask_Mdiff_truth)\n\u001b[1;32m    214\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m             loss_Mdiff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMdiff_model\u001b[39m.\u001b[39;49mforward(x_Mdiff[jb], cond_inp_Mdiff, mask_Mdiff_truth)\n\u001b[1;32m    216\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(loss_Ntot \u001b[39m+\u001b[39m loss_M1 \u001b[39m+\u001b[39m loss_M1reg \u001b[39m+\u001b[39m loss_Mdiff)\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/ceph/users/spandey/AR_NPE/nf/all_models.py:642\u001b[0m, in \u001b[0;36mNSF_Mdiff_CNNcond.forward\u001b[0;34m(self, x_inp, cond_inp, mask)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39m# if len(x.shape) > 1:\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m#     x = x[:, 0]\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m log_det_all_jd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(x_inp\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    643\u001b[0m log_det_all_jd \u001b[39m=\u001b[39m log_det_all_jd\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    644\u001b[0m \u001b[39mfor\u001b[39;00m jf \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnflows):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepochs_Ntot_only = config_train['nepochs_Ntot_only']\n",
    "nepochs_Ntot_M1_only = config_train['nepochs_Ntot_M1_only']\n",
    "nepochs_all = config_train['nepochs_all']\n",
    "\n",
    "\n",
    "nepochs_array = [nepochs_Ntot_only, nepochs_Ntot_M1_only, nepochs_all]\n",
    "train_Ntot_array = [1]\n",
    "train_M1_array = [1]\n",
    "train_Mdiff_array = [1]\n",
    "\n",
    "\n",
    "for jn in (range(len(nepochs_array))):\n",
    "    loss_min = 1e20\n",
    "    torch.cuda.empty_cache()\n",
    "    ninit = 0\n",
    "    nepochs = nepochs_array[jn]\n",
    "    if nepochs > 0:\n",
    "        train_Ntot = train_Ntot_array[jn]\n",
    "        train_M1 = train_M1_array[jn]\n",
    "        train_Mdiff = train_Mdiff_array[jn]\n",
    "\n",
    "        if jn > -1:\n",
    "            print('loading bestfit model')\n",
    "            bestfit_model = (torch.load(save_bestfit_model_name))\n",
    "            model.load_state_dict(bestfit_model['state_dict'])\n",
    "            optimizer.load_state_dict(bestfit_model['optimizer'])\n",
    "            scheduler.load_state_dict(bestfit_model['scheduler'])\n",
    "            # loss_min = bestfit_model['loss_min']\n",
    "            loss = bestfit_model['loss']\n",
    "            lr = bestfit_model['lr']\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = learning_rate\n",
    "\n",
    "        for jt in tqdm(range(nepochs)):\n",
    "            for jd in range(len(dataloader)):\n",
    "                torch.cuda.empty_cache()\n",
    "                optimizer.zero_grad()\n",
    "                cond_tensor_jd, cond_tensor_nsh_jd, mask_tensor_M1_train_jd, mask_tensor_Mdiff_train_jd, X_M1_jd, \\\n",
    "                    X_Nhalo_jd, X_Mdiff_jd, Nhalos_truth_tensor_jd, mask_tensor_M1_train_FP_jd, X_M1_FP_jd = next(iter(dataloader))\n",
    "\n",
    "                if cond_sim == 'quijote':               \n",
    "                    mask_tensor_M1_train_FP_jd = None\n",
    "                    X_M1_FP_jd = None\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                if 1-all_gpu:\n",
    "                    cond_tensor_jd = cond_tensor_jd.cuda(dev)\n",
    "                    cond_tensor_nsh_jd = cond_tensor_nsh_jd.cuda(dev)\n",
    "                    mask_tensor_M1_train_jd = mask_tensor_M1_train_jd.cuda(dev)\n",
    "                    mask_tensor_Mdiff_train_jd = mask_tensor_Mdiff_train_jd.cuda(dev)\n",
    "                    X_M1_jd = X_M1_jd.cuda(dev)\n",
    "                    X_Nhalo_jd = X_Nhalo_jd.cuda(dev)\n",
    "                    X_Mdiff_jd = X_Mdiff_jd.cuda(dev)\n",
    "                    Nhalos_truth_tensor_jd = Nhalos_truth_tensor_jd.cuda(dev)\n",
    "                    if mask_tensor_M1_train_FP_jd is not None:\n",
    "                        mask_tensor_M1_train_FP_jd = mask_tensor_M1_train_FP_jd.cuda(dev)\n",
    "                        X_M1_FP_jd = X_M1_FP_jd.cuda(dev)\n",
    "                    torch.cuda.empty_cache()\n",
    "                                \n",
    "                loss = model(\n",
    "                    X_Mdiff_jd,\n",
    "                    X_M1_jd,\n",
    "                    X_Nhalo_jd,\n",
    "                    cond_x=cond_tensor_jd,\n",
    "                    cond_x_nsh=cond_tensor_nsh_jd,\n",
    "                    mask_Mdiff_truth_all=mask_tensor_Mdiff_train_jd,\n",
    "                    mask_M1_truth_all=mask_tensor_M1_train_jd,\n",
    "                    Nhalos_truth_all=Nhalos_truth_tensor_jd,\n",
    "                    use_Ntot_samples=False,\n",
    "                    use_M1_samples=False,\n",
    "                    train_Ntot=train_Ntot,\n",
    "                    train_M1=train_M1,\n",
    "                    train_Mdiff=train_Mdiff,\n",
    "                    # x_Mdiff_FP=X_Mdiff_FP,\n",
    "                    x_M1_FP=X_M1_FP_jd,\n",
    "                    # x_Ntot_FP=X_Nhalo_FP,\n",
    "                    # Nhalos_truth_all_FP=Nhalos_truth_tensor_FP,\n",
    "                    # mask_Mdiff_truth_all_FP=mask_tensor_Mdiff_train_FP,\n",
    "                    mask_M1_truth_all_FP=mask_tensor_M1_train_FP_jd             \n",
    "                    )\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            epoch_tot_counter += 1\n",
    "            if (np.mod(jt, int(nepochs / 100)) == 0) or (jt == nepochs - 1):\n",
    "                if float(loss.cpu().detach().numpy()) < loss_min:\n",
    "                    loss_min = float(loss.cpu().detach().numpy())\n",
    "                    print('saving bf at:', ', with loss:', np.round(loss_min, 5), ', at epoch:', jt, \n",
    "                          'learning rate:', optimizer.param_groups[0]['lr'], ', train_Ntot:', train_Ntot, \n",
    "                          'train_M1:', train_M1, ', train_Mdiff:', train_Mdiff, ', epoch_tot_counter:', epoch_tot_counter)\n",
    "                    lr=optimizer.param_groups[0]['lr']\n",
    "                    # print(loss_min, lr)\n",
    "                    state = {'loss_min': loss_min, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                              'scheduler': scheduler.state_dict(), 'loss':loss, 'lr':lr, 'epoch_tot_counter':epoch_tot_counter}\n",
    "\n",
    "                    torch.save(\n",
    "                        state, save_bestfit_model_name\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
